{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "\n",
    "The original cell dataset is viper_ds.csv located in the ./dataset directory and requires 7GB of space. \n",
    "To load the entire viper_ds dataset more than 16GB of memory are required. Since my local machine doesn't meet the requirements, and for the purpose of this test, we will work with subsamples. \n",
    "\n",
    "These subsamples contain n [10000,100000,1000000] samples.\n",
    "\n",
    "The original vipe_ds.csv contains roughtly 36 Mio rows of cell data, where each row represents a single cell.\n",
    "The first half (18Mio) are labeled as bad cells, since these are the cells which interfere the simulation and cause problems during the sumulation convergence.\n",
    "\n",
    "The second half (18Mio) are labeled as good cells, which are all the cells which cause no problems during the sumulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify a list of samplesizes that will create our subsamples.\n",
    "\n",
    "We use pandas to read the original dataset. Pandas offers a parameter to limit the number of rows to be read from the file. (nrows)\n",
    "\n",
    "We retrieve a subsample from the section of bad cells and a subsample from the section of good cells. To acheive this, we skip the header from the section of bad cells and then start reading until we satisfy the desired samplesize.\n",
    "\n",
    "We repeat this step and skip 20Mio rows from the entire set to be somewhere located within the section of good cells and start counting from here. \n",
    "\n",
    "With this approach we can make sure, that we collect n/2 bad samples and n/2 good samples from the main dataset.\n",
    "\n",
    "\n",
    "After mergin the two sets of bad and good cells we save the DataFrame to the local disk as a csv file. This will be done via the df.to_csv() command.\n",
    "\n",
    ". \n",
    "\n",
    "Apache Arrow is an ideal in-memory transport layer for data that is being read or written with Parquet files. \n",
    "The Apache Parquet project provides a standardized open-source columnar storage format for use in data analysis systems.\n",
    "\n",
    "https://arrow.apache.org/docs/python/parquet.html\n",
    "\n",
    "\n",
    "\n",
    "Next we also want to save the combined DataFrame to the Apache Parquet Format.\n",
    "This requires the DataFrame to be converted to the pyarrow.Table object. We can achieve this via the pa.Table.from_pandas() function.\n",
    "\n",
    "\n",
    "The PyArrow Table type is not part of the Apache Arrow specification, but is rather a tool to help with wrangling multiple record batches and array pieces as a single logical dataset.\n",
    "\n",
    "\n",
    "Finally, we write the table to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [10000,100000,1000000]\n",
    "\n",
    "header = pd.read_csv(\"./dataset/viper_ds.csv\", nrows=1)\n",
    "\n",
    "for nc in n:\n",
    "    df_badcells = pd.read_csv(\"./dataset/viper_ds.csv\", skiprows= 1,nrows=nc/2)\n",
    "    df_goodcells = pd.read_csv(\"./dataset/viper_ds.csv\", skiprows= 20000000,nrows=nc/2)\n",
    "    \n",
    "    df_badcells.columns = header.columns\n",
    "    df_goodcells.columns = header.columns\n",
    "    \n",
    "    combined = pd.concat([header, df_badcells,df_goodcells])\n",
    "    combined.reset_index(drop=True)\n",
    "        \n",
    "    ## write dataframe to csv\n",
    "    combined.to_csv(\"./dataset/cell_data_\"+str(nc)+\".csv\", index=False)\n",
    "    \n",
    "    ## wirte dataframe to parquet\n",
    "    py_arrow_combined = pa.Table.from_pandas(combined)\n",
    "    pq.write_table(py_arrow_combined, \"./dataset/cell_data_\"+str(nc)+\"_arrow.parquet\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the performance of reading the new parquet files in comparison to the csv files.\n",
    "\n",
    "For this test we apply four functions:\n",
    "\n",
    " - pandas read_csv()\n",
    " - pyarrow.parquet read_table()\n",
    " - pyarrow.parquet read_pandas()\n",
    " - pyarrow csv()\n",
    " \n",
    "We will measure the time duration reading in the files.\n",
    "\n",
    "For reading a parquet file, pyarrow offers two function. (read_table(), read_pandas())\n",
    "\n",
    "In practice, a Parquet dataset may consist of many files in many directories. We can read a single file back with read_table.\n",
    "\n",
    "When reading a file that used a Pandas dataframe as the source, we use read_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs(nc):\n",
    "    l_times = []\n",
    "    \n",
    "    # standard pandas read csv\n",
    "    start = time.time()\n",
    "    header = pd.read_csv('./dataset/cell_data_'+str(nc)+'.csv')\n",
    "    end = time.time()\n",
    "    l_times.append(end-start)\n",
    "    \n",
    "    # apyrrow read parquet table file\n",
    "    start = time.time()\n",
    "    table2 = pq.read_table('./dataset/cell_data_'+str(nc)+'_arrow.parquet').to_pandas()\n",
    "    end = time.time()\n",
    "    l_times.append(end-start)\n",
    "    \n",
    "    # pyarrow read parquet file from a pandas original source\n",
    "    start = time.time()\n",
    "    table3 = pq.read_pandas('./dataset/cell_data_'+str(nc)+'_arrow.parquet').to_pandas()\n",
    "    end = time.time()\n",
    "    l_times.append(end-start)\n",
    "    \n",
    "    #pyarrow read csv\n",
    "    start = time.time()\n",
    "    table4 = csv.read_csv('./dataset/cell_data_'+str(nc)+'.csv').to_pandas()\n",
    "    end = time.time()\n",
    "    l_times.append(end-start)\n",
    "        \n",
    "    return pd.Series(l_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10000</th>\n",
       "      <th>100000</th>\n",
       "      <th>1000000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pandas read_csv</th>\n",
       "      <td>0.026139</td>\n",
       "      <td>0.225137</td>\n",
       "      <td>2.013270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyarrow from table</th>\n",
       "      <td>0.072070</td>\n",
       "      <td>0.054328</td>\n",
       "      <td>0.120114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyarrow from pandas</th>\n",
       "      <td>0.007632</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>0.089052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyarrow read_csv</th>\n",
       "      <td>0.014740</td>\n",
       "      <td>0.051877</td>\n",
       "      <td>0.516226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      10000     100000    1000000\n",
       "pandas read_csv      0.026139  0.225137  2.013270\n",
       "pyarrow from table   0.072070  0.054328  0.120114\n",
       "pyarrow from pandas  0.007632  0.015180  0.089052\n",
       "pyarrow read_csv     0.014740  0.051877  0.516226"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timings_df = pd.DataFrame()\n",
    "for nc in n:\n",
    "    timings_df = pd.concat([timings_df, load_dfs(nc)], axis = 1)\n",
    "\n",
    "index = ['pandas read_csv', 'pyarrow from table', 'pyarrow from pandas', 'pyarrow read_csv']\n",
    "\n",
    "timings_df.index = index\n",
    "timings_df.columns = n\n",
    "\n",
    "timings_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
